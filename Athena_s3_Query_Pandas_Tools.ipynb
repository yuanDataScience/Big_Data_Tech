{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for Querying AWS S3 to Pandas Dataframe by Presto Using Athena\n",
    "Author: Yuan Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Athena allows users to run presto query for querying big data. The query results can be stored on AWS s3, which can be downloaded or queried later. This notebook includes several routinely used functions implemented in Python that allow to either save query results to local storage, which can be read by pandas dataframe, or directly transfer query results to pandas dataframe for further data analysis. There are several ways and packages that can be used. This notebook focused on using contextlib2, pyathenajdbc, and boto3 packages. \n",
    "\n",
    "First, let's import the necessary packages and define the parameters including AWS access key, secret access key, s3 bucket name, s3 path, the region of your AWS account. All these parameters can be saved as environment variables and imported to notebook without exposing these sensitive information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import contextlib2\n",
    "from pyathenajdbc import connect\n",
    "from pyathenajdbc.util import as_pandas\n",
    "import boto3\n",
    "from boto3 import Session\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Import AWS account and s3 information\n",
    "aws_access_key=os.environ[\"AWS_KEY\"]\n",
    "aws_secret_access_key=os.getenv(\"AWS_SECRET_KEY\")\n",
    "\n",
    "bucket_name=os.getenv(\"AWS_s3_bucket\")\n",
    "s3_path=os.getenv(\"AWS_s3_path\")\n",
    "output_file=\"test.csv\"\n",
    "region=os.getenv(\"AWS_region\")\n",
    "staging_dir='s3://'+bucket_name+'/'+s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data query and transfer using pyathenajbdc connect\n",
    "### 1.1 Pyathenajdbc connect with s3 bucket download_file command\n",
    "pyathenajdbc package provides a convenient way to directly set up a connection to AWS s3 storage, execute the presto query, and store the query results to s3 path assigned by the user. Specifically, this method consists of the following steps:\n",
    "1. initialize a pyathenajbdc connect object using AWS account and s3 information, including AWS access key, secret access key, s3 bucket name and path, region name.\n",
    "2. initialze a cursor object from the connect object\n",
    "3. execute presto query using the cursor.execute(presto_sql) command\n",
    "4. obtain the location of the query result file using s3 path and the query_id fetched by the cursor\n",
    "5. initialize a s3 resource using the AWS account information\n",
    "6. download the file by the s3 resource\n",
    "\n",
    "The following function: athena_query_s3_csv implemented steps 1 to 6 and allows the users to execute presto query by Athena on AWS and save the query results to local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def athena_query_s3_csv(athena_sql,bucket_name,s3_path,region,output_file,access_key,secret_key):\n",
    "    \"\"\"\n",
    "    This function execute the presto query, store the results in the s3 location \n",
    "    designated by bucket_name and s3_path, and download the results to local machine\n",
    "    \n",
    "    Inputs:\n",
    "      athena_sql:  the presto query string\n",
    "      bucket_name: s3 bucket name for result storage\n",
    "      s3_path:     the path of the s3 storage\n",
    "      region:      the region of the AWS account\n",
    "      output_file: the name of the output file when downloading results to local machine\n",
    "      access_key:  AWS access key\n",
    "      secret_key:  AWS secret access key\n",
    "    Output:\n",
    "      None. Query results is saved as output_file on local machine.\n",
    "    \"\"\"\n",
    "    staging_dir='s3://'+bucket_name+'/'+s3_path\n",
    "    conn = connect(access_key = aws_access_key,\n",
    "               secret_key = aws_secret_access_key,\n",
    "               s3_staging_dir = staging_dir,\n",
    "               region_name = region)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(athena_sql)\n",
    "    key = s3_path + cursor.query_id + '.csv'\n",
    "    s3 = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "    s3.Bucket(bucket_name).download_file(key, output_file)\n",
    "    for s in s3.Bucket(bucket_name).objects.filter(Prefix = key):\n",
    "        s.delete()\n",
    "        \n",
    "athena_query_s3_csv(athena_sql,bucket_name,\n",
    "                    s3_path,region,output_file,\n",
    "                    aws_access_key,aws_secret_access_key)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pyathenajdbc connect directly converted to pandas dataframe\n",
    "A more straightforward and simple way is to directly input the pyathena connect and presto query string to the read_sql command of pandas. The query results will be directly save as pandas dataframe. As shown in the following function, athena_query_df_sql():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def athena_query_df_sql(athena_sql,staing_dir,region,access_key,secret_key):\n",
    "    \"\"\"\n",
    "    This function execute the presto query using AWS Athena, and fetch the \n",
    "    results directly to a pandas dataframe\n",
    "    \n",
    "    Inputs:\n",
    "      athena_sql:  the presto query string\n",
    "      staging_dir: the s3 bucket name + s3 path\n",
    "      region:      AWS account region\n",
    "      access key:  AWS access id\n",
    "      secret key:  AWS access secret key\n",
    "    Output:        A pandas dataframe containing query results  \n",
    "    \"\"\"\n",
    "    conn = connect(access_key = access_key,\n",
    "               secret_key = secret_key,\n",
    "               s3_staging_dir = staing_dir,\n",
    "               region_name = region)\n",
    "    return pd.read_sql(athena_sql,conn)\n",
    "\n",
    "athena_query_df_sql(athena_sql,staging_dir,region,aws_access_key,aws_secret_access_key).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data query and transfer using boto3 and athena client with paginator\n",
    "This method directly uses the boto3 functionailities with athena client without using third party packages such as pyathena. It is much easier and straightforward to install boto3, especially if you are using conda to manage your packages. In addition, athena client can use paginator to partition the results. However, compared to pyathenajdbc methods, to use this method, you need to understand the athena client class a little bit, which is not very straightforward. The steps of the method is the following:\n",
    "1. initialize a boto3 session using AWS account inforamtion\n",
    "2. initialize an athena client\n",
    "3. execute the presto query with a designated s3 staging postion, and fetch the query_id\n",
    "4. repeated check the query_status. If is success, paginate the query results using the get_paginator method,\n",
    "   with 'get_query_results' method name as the input\n",
    "5. fetch the paginated results to results_iter variable\n",
    "6. iterate the results_iter and append results to a list, which is data_list in the function\n",
    "7. iterate the data_list and extract value for each column to result_data list\n",
    "8. tanspose the data_list, and convert it to dictionary by combining with column names\n",
    "9. convert the dictionary to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "session=boto3.Session(aws_access_key_id=aws_access_key,aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "def athena_to_df(session,region,athena_sql,staging_dir):\n",
    "    \"\"\"\n",
    "    This function execute presto query and fetch the results to a pandas dataframe\n",
    "    \n",
    "    Inputs:\n",
    "      session:     a boto3 session\n",
    "      region:      AWS account region\n",
    "      athena_sql:  presto query string\n",
    "      staging_dir: s3 bucket name + s3 path\n",
    "      \n",
    "    Output:\n",
    "      a pandas dataframe with the query results      \n",
    "    \"\"\"\n",
    "    query_status=None\n",
    "    page_index=0\n",
    "    row_index=0\n",
    "    data_list=[]\n",
    "    result_data=[]\n",
    "    \n",
    "    # initialize an athena client\n",
    "    client=session.client('athena',region)\n",
    "    \n",
    "    # execute the presto query, and fetch the query_id\n",
    "    query_id=client.start_query_execution(QueryString=athena_sql,\n",
    "                                     ResultConfiguration={\n",
    "                                         'OutputLocation':staging_dir\n",
    "                                     })['QueryExecutionId']\n",
    "    \n",
    "    # repeatedly check the query status\n",
    "    while query_status=='QUEUED' or query_status=='RUNNING' or query_status is None:\n",
    "        query_status=client.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']\n",
    "        if query_status=='FAILED' or query_status=='CANCELLED':\n",
    "            raise Exception('Athena query with the string \"{}\" failed or was cancelled'.format(athena_sql))\n",
    "        time.sleep(10)\n",
    "        \n",
    "    # paginate the results, and save the paginated results to results_iter\n",
    "    results_paginator=client.get_paginator('get_query_results')\n",
    "    results_iter=results_paginator.paginate(\n",
    "                   QueryExecutionId=query_id,\n",
    "                   PaginationConfig={\n",
    "                      'PageSize': 1000\n",
    "                   }\n",
    "                 )\n",
    "\n",
    "    # iterate the results_iter and extract column name and column values to \n",
    "    # column_head and data_list, respectively\n",
    "    for page in results_iter:\n",
    "        start_row=0\n",
    "        if page_index==0:\n",
    "            column_head = [col['Label'] for col in page['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n",
    "        for row in page['ResultSet']['Rows']:\n",
    "            if start_row==0 and page_index==0:\n",
    "                start_row+=1\n",
    "                continue\n",
    "            data_list.append(row['Data'])\n",
    "        page_index+=1 \n",
    "    \n",
    "    # iterate the data_list, extract the column values and transpose the results\n",
    "    # as a list\n",
    "    for record in data_list:\n",
    "        result_data.append([x.get('VarCharValue','') for x in record])\n",
    "    result_data=np.array(result_data).transpose().tolist() \n",
    "    \n",
    "    # combine the column name and column values, and convert them to a dictionary\n",
    "    # then convert the dirctionary to the pandas dataframe to return\n",
    "    return pd.DataFrame.from_dict(dict(zip(column_head,result_data)))\n",
    "\n",
    "tdf=athena_to_df(session,region,athena_sql,staging_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

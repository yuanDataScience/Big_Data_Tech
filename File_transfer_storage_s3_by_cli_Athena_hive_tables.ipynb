{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Transfer in AWS S3 by AWS Cli and File Storage by Hive Tables Using Athena\n",
    "Author: Yuan Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook consists of two sections. The first section discussed how to transfer files between s3 folders using AWS cli. it also includes the procedures to save the log files during the file transfer and compare the file names between the soruce and destination file folders to make sure all the files in source s3 folders are transferred to the destination folder.\n",
    "\n",
    "The second section is related to file storage. AWS s3 is commonly used for file storage. Together with other AWS services, it can be seamlessly integrated with other big data technologies, including presto query and spark. AWS athena supports a subset of presto functions for querying big volume of data. In addition, Athena provides the functions to establish hive tables that projects the table schema to s3 files in various data formats, including csv, gzip and parquet. This notebook will show scripts to create hive tables using AWS athena. In addition, AWS GLUE can also be used to gether the table catelogue information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. s3 file transfer\n",
    "### 1a. File transfer by AWS cli\n",
    "#### 1. to transfer all files from one directory to another directory\n",
    "```\n",
    "aws s3 cp source_folder dest_folder --recursive --profile (aws profile of the destination s3 bucket)\n",
    "```\n",
    "#### 2. to transfer individual files:\n",
    "```\n",
    "aws s3 cp source_file dest_file --profile (aws profile of the destination file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Using shell script with log files for the transfer of large volumes of s3 files\n",
    "Log files will provide important information on file transfer and help to find the missing files during the transfer. In addition, when large volumes of files are transferred, it is better to do the file transfer in the backgroud. Therefore, it is a common practice to use shell scripts for file transfer and output the script execution information in a log file. The following procedure can be used for such a data transfer:\n",
    "#### 1. create the shell script for file transfer:\n",
    "\n",
    "The following is the shell script fileï¼š\n",
    "```\n",
    "#!/usr/bin/bash\n",
    "\n",
    "# To copy an entire directory to another directory\n",
    "aws s3 cp source_folder dest_folder --recursive --profile (aws profile of the destination s3 bucket)\n",
    "\n",
    "# To copy an individual file to another file\n",
    "aws s3 cp source_file dest_file --profile (aws profile of the destination file)\n",
    "```\n",
    "#### 2. save the shell file as abc.sh\n",
    "#### 3. cd to the folder containing abc.sh, and run the following shell commands:\n",
    "```\n",
    "chmod 755 abc.sh\n",
    "nohup ./abc.sh > output_log.txt &\n",
    "```\n",
    "By doing this, large volumes of files are transferred in the background of the server. You can log off the server without impactig the file transfer process.\n",
    "\n",
    "#### 4. check the output_log.txt for the log files of the file transfer. \n",
    "The common error messages include:\"access_denied\", \"no such file or directory\", \"copy failed:\", and \"fatal error: could not connect to the endpoint URL:\". You can use grep to check if these error messages are presented in the log files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Comparing file names between source and destination s3 folders\n",
    "After s3 files are transferred from source to destination folders, it will be useful to compare the file names in the soruce and destination folders, and make sure the file names contained in these folders are consistent.\n",
    "This procedure first outputs the file names in source and destination s3 folders to source_file.txt and dest_file.txt, respectively, and then compares the contents of these two files to find out which files are missing:     \n",
    "```\n",
    "aws s3 ls $source --recursive | awk '{$1=$2=$3=\"\";print $0}' | sed 's/^[ \\t]*//' | sort > source_file.txt\n",
    "aws s3 ls $dest --recursive | awk '{$1=$2=$3=\"\";print $0}' | sed 's/^[ \\t]*//' | sort > dest_file.txt\n",
    "echo $(diff source_file.txt dest_file.txt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Computing size of all the files in a s3 folder\n",
    "```\n",
    "aws s3 ls s3_folder --recursive | awk 'BEGIN {total=0} {total+=$3} END {print total/1024/1024\" MB\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File storage and query \n",
    "It is a common case that huge volumes of s3 files are stored in one s3 folder. These s3 files are stored in a consistent format, such as csv, gzip, json, or parquet, and all these s3 files have a consistent schema. Therefore, all s3 files contained in the same s3 folder can be treated as a huge data table. In AWS, athena and GLUE can be used to define and establish such data tables based on these s3 files. After such tables are established, big data techniques such as presto query and spark can be used to query and manipulate the big data in these tables.\n",
    "### 2a. build hive tables in Athena using boto3:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sql = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS table_name (\n",
    "`id` int,\n",
    "`name` string,\n",
    "`quantity` bigint,\n",
    "`percentage` double,\n",
    "`cost` float\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.lazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES(\n",
    "  'serialization.format' = ',',\n",
    "  'field.delim' = ','\n",
    ") LOCATION 's3://bucket/gz folder/'\n",
    "TBLPROPERTIES ('has_encrypted_data'='false');\n",
    "\"\"\"\n",
    "\n",
    "parquet_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS table_name\n",
    "WITH (\n",
    "  external_location = 's3://bucket/parquet_folder',\n",
    "  format = 'PARQUET',\n",
    "  parquet_compression = 'SNAPPY',\n",
    "  partitioned_by = ARRAY['protein']\n",
    ") AS SELECT * FROM\n",
    "gzip_partitioned_table;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_KEY\")\n",
    "region = os.getenv(\"AWS_REGION\")\n",
    "staging_dir = os.getenv(\"AWS_STAGING_DIR\")\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_access_key)\n",
    "client = session.client('athena',region)\n",
    "\n",
    "client.start_query_execution(QueryString=table_sql,\n",
    "                            ResultConfiguration={'OutputLocation':staging_dir})\n",
    "\n",
    "client.start_query_execution(QueryString=parquet_sql,\n",
    "                            ResultConfiguration={'OutputLocation':staging_dir})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
